# LectorPlus
**Lector** is an extraction tool originated from a joint research project between Roma Tre University and University of Alberta. The tool is able to extract facts from English Wikipedia article text, learning the expressions (i.e. phrases) that are commonly used to describe instances of relations between named entities in the text. It reaches an estimated precision of 95% in its first version. 

**LectorPlus** is an extension in which the tool has been applied to different languages, other than English. It is able to extract facts for Spanish, Italian, French and German version of Wikipedia and focuses primarily on DBPedia as a reference Knowledge Graph.

More information is available about the project at the Lector homepage: http://www.dia.uniroma3.it/db/lector/

## Approach
Each execution of the tool performs a first pass over the whole dump harvesting the phrases that are commonly used to describe instances of DBPedia properties in the text (e.g. `[Book] written by [Writer]` describes an instance of the property `writer`). Then, in a second pass, the tool uses the harvested phrases to extracts new instances of such properties involving named entities that were not related before.


## Getting Started

To execute LectorPlus on your machine you should have installed:
- JDK 1.8
- [Apache Maven](https://maven.apache.org/)
- command line tool:  **wget** and **git**

### Clone the project

First of all, clone the project in your local folder using:
```
git clone https://github.com/miccia4/LectorPlus.git
```

### Setting up the environment

The tool takes as input a Wikipedia XML dump (in one of the language above) and outputs several NTriples files with the triples that have been extracted. 

- In order to run the tool on specific versions of Wikipedia please edit the file:
	 ```
	 dumps.properties
	 ```
	it lists the specific URLs of the input Wikipedia dumps. We alrready filled it with the complete dumps of Feb. 2017 in all the languages above but other versions can be easily used (from https://dumps.wikimedia.org/).

- Also, in order to simplify the download of those dumps and the picking up of the other necessary files we provide a script which creates the folders and set up the environment used by LectorPlus. 
	
	Run once our install script:
	```
	sh install.sh
	```
	It will take some time<sup>*</sup> (many files to downlaod) but at the end it will create the root folder `/data` described below.
	<sup>*</sup>: Note that the English Wikipedia dump only is around 12GB! 

#### Structure of the folder `/data`
The folder `/data` contains a list of sub-folders and includes all the necessary files. The languages inside parenthesis means that the content of the folder is repeated for all of them.

	|-- input (en es it de fr):									
	|		|-- wikipedia: it contains the initial dump of Wikipedia
	|		|-- dbpedia: it contains the Mappingbased Objects dump of DBPedia (used as a reference)
	|
	|-- languages (en es it de fr): it contains the properties of each language used by the parser
	|
	|-- lists (en es it de fr): used by the parser to filter out undesired named entities
	|		|-- currencies.tsv
	|		|-- nationalities.tsv
	|		|-- professions.tsv
	|
	|-- models (en): OpenNLP models that are used from the English parser.
	|		|-- en-lemmatizer.dict
	|		|-- en-pos-maxent.bin
	|		|-- en-token.bin
	|
	|-- sources (en es it de fr): other important files used in the process
	|		|-- type: it contains the instance types, or other dictionaries (when present)
	|		|-- redirect.tsv: it contains tsv files used to solve redirect names during the parsing

Other folders are created at run-time:

	|-- index: it will contains the Lucene index of DBPedia MappingBased objects, redirects and types
	|
	|-- lector: it will contains a csv file with the phrases used by LectorPlus, for each language
	
### Build and run

After created the folder `/data` you are ready to build a executable version of LectorPlus, using:
```
maven clean install
```

and running it using the following command:

```
sh run.sh <output_folder>
```
It takes the path of the output folder as a parameter and executes the extraction from all the Wikipedia dumps listed in `dumps.properties` file.
The output folder will contain all the compressed NTriples files produced by the tool.


## Details and contacts
More details can be found in the paper:

>  "Accurate Fact Harvesting from Natural Language Text in Wikipedia with Lector."   
>  by Matteo Cannaviccio, Denilson Barbosa and Paolo Merialdo.   

The paper was presented at the "19th International Workshop on the Web and Databases (WebDB 2016)" 
(http://webdb2016.technion.ac.il/program.html).

If you have any questions, please feel free to contact the authors.

- Matteo Cannaviccio (cannaviccio@uniroma3.it)
- Denilson Barbosa (denilson@ualberta.ca)
- Paolo Merialdo (merialdo@uniroma3.it)
